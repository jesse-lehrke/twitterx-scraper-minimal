{
  "cells": [
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "1. Install Chrome\n",
        "2. Check your Chrome version\n",
        "3. Install Chromedriver version appropriate for your Chrome version\n",
        "4. Install requirements\n",
        "    - If I find time, I will make a requirements.txt, but I am running this script in a larger venv for another project, so can't quickly pip freeze one\n",
        "\"\"\""
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from seleniumwire import webdriver\n",
        "from seleniumwire.utils import decode\n",
        "\n",
        "from webdriver_manager.chrome import ChromeDriverManager\n",
        "\n",
        "from selenium.webdriver.chrome.service import Service\n",
        "\n",
        "from selenium.webdriver.support.ui import WebDriverWait\n",
        "from selenium.webdriver.support import expected_conditions as EC\n",
        "\n",
        "from selenium.webdriver.common.by import By\n",
        "from selenium.webdriver.common.action_chains import ActionChains\n",
        "from selenium.webdriver.common.keys import Keys\n",
        "\n",
        "\n",
        "import time\n",
        "\n",
        "import pandas as pd"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "xVR93EXfO3RQ"
      },
      "source": [
        "# Advanced search"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QW-C_Js5O3RR"
      },
      "outputs": [],
      "source": [
        "# IIRC, must use this advanced search url  with ending \"f=live\". Otherwise first returns \"top\" list, not \"latest\", meaning scrape will stop too soon\n",
        " \n",
        "ACCOUNT = \"IAPonomarenko\"\n",
        "START = \"2023-01-01\" #YYYY-MM-DD\n",
        "END = \"2023-02-01\" #YYYY-MM-DD\n",
        "url_adv = f\"https://twitter.com/search?q=(from%3A{ACCOUNT})%20until%3A{END}%20since%3A{START}&src=typed_query&f=live\"\n",
        "\n",
        "print(url_adv)\n",
        "# TO scale, could loop through a list of account, start, ends dates.\n",
        "\n",
        "\n",
        "# Sample urls for testing\n",
        "# url_adv = \"https://twitter.com/search?q=(from%3AIAPonomarenko)%20until%3A2023-05-31%20since%3A2023-01-01&src=typed_query\"\n",
        "# url_adv = \"https://twitter.com/search?q=(from%3AIAPonomarenko)%20until%3A2023-02-31%20since%3A2023-01-01&src=typed_query&f=live\"\n",
        "# url_adv = \"https://twitter.com/search?q=(from%3AHDeSotoPeru)%20until%3A2021-04-30%20since%3A2021-01-01&src=typed_query&f=live\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZjzguQWuO3RR"
      },
      "outputs": [],
      "source": [
        "# ATTENTION:\n",
        "# On first run, enter your Twitter username and password. On next run this will be saved to the \"user-data-dir\"\n",
        "# This is because to use Twitter's advanced search options, you need to be logged in (update: well, now you always need to be logged in)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7gEacIf6O3RS",
        "outputId": "9f84ddff-4014-49cb-96fb-055861479987"
      },
      "outputs": [],
      "source": [
        "# Change this based on where you put your Chromedriver. I like to put it close to my script (do not need to specify this if Chromedriver is in your PATH iirc)\n",
        "driver_location = '../INPUT/chromedriver' \n",
        "\n",
        "# Add options\n",
        "options = webdriver.ChromeOptions()\n",
        "\n",
        "options.add_argument(\"--start-maximized\")\n",
        "options.add_argument(\"--disable-dev-shm-usage\")\n",
        "options.add_argument(\"--no-sandbox\")\n",
        "\n",
        "options.add_argument(\"user-data-dir=C:\\environments\\selenium\")\n",
        "#options.add_argument(\"user-data-dir=selenium\") # I usually use this and put my directory next to my work, not the above (which I added for a friend)\n",
        "\n",
        "options.add_argument(\"user-agent=Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/113.0.0.0 Safari/537.36\")\n",
        "options.add_argument(\"--disable-popup-blocking\")\n",
        "options.add_argument(\"--disable-infobars\")\n",
        "options.add_argument(\"--disable-extensions\")\n",
        "options.add_argument('--disable-blink-features=AutomationControlled')\n",
        "\n",
        "\n",
        "# Start driver\n",
        "driver = webdriver.Chrome(service=Service(driver_location), options=options)\n",
        "\n",
        "# Run after driver initialized\n",
        "driver.execute_script(\"Object.defineProperty(navigator, 'webdriver', {get: () => undefined})\")\n",
        "\n",
        "driver.get(url_adv)\n",
        "\n",
        "time.sleep(5) # might want to make these random\n",
        "\n",
        "counter = 0\n",
        "index = None\n",
        "index_list = []\n",
        "\n",
        "new_list = []\n",
        "\n",
        "# This is how we scroll now! Will scroll until the end, using screen size to see if there is more\n",
        "lastHeight = driver.execute_script(\"return document.documentElement.scrollHeight\")\n",
        "print('lastHeight', lastHeight)\n",
        "\n",
        "while True:\n",
        "\n",
        "    driver.execute_script(f\"window.scrollTo(0, {lastHeight});\")\n",
        "    time.sleep(1)\n",
        "    newHeight = driver.execute_script(\"return document.documentElement.scrollHeight\")\n",
        "    print('newHeight', newHeight)\n",
        "\n",
        "    if newHeight == lastHeight:\n",
        "        break\n",
        "\n",
        "    lastHeight = newHeight\n",
        "\n",
        "    elem = driver.find_element(By.TAG_NAME, \"html\")\n",
        "    elem.send_keys(Keys.END)\n",
        "    print(\"Scrolling\")\n",
        "    time.sleep(5) # might want to make these random\n",
        "\n",
        "\n",
        "    for request in driver.requests:\n",
        "        request_str = str(request)\n",
        "\n",
        "        if \"api/graphql\" in request_str and \"twitter.com\" in request_str and \"rawQuery\" in request_str:\n",
        "            print(request_str)\n",
        "            try:\n",
        "                data = decode(request.response.body, request.response.headers.get('Content-Encoding', 'identity'))\n",
        "\n",
        "                data = data.decode(\"utf8\") #GB2312\n",
        "\n",
        "                print(\"Index of object \" + str(counter))\n",
        "\n",
        "                index = counter\n",
        "                index_list.append(index)\n",
        "                print(\"----------------------------------------------\")\n",
        "\n",
        "                # # Getting request we want\n",
        "                new_list.append(data)\n",
        "\n",
        "\n",
        "            except:\n",
        "                print(\"ERROR DECODING\")\n",
        "                print(\"----------------------------------------------\")\n",
        "\n",
        "        counter += 1\n",
        "\n",
        "driver.close()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Parse results\n",
        "\n",
        "This is allot of exploring the json to find the right data. You can skip down to FINAL PARSE if you like"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Jotr9c62O3RU"
      },
      "outputs": [],
      "source": [
        "import json"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lQEQQXweO3RT",
        "outputId": "0c32d67b-ec14-4593-c6f2-6afb2a51a52e"
      },
      "outputs": [],
      "source": [
        "len(new_list)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mOsVN5sWO3RT",
        "outputId": "e3a7cf61-6f04-46c7-dacd-39401534c88d"
      },
      "outputs": [],
      "source": [
        "new_list[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cg21-GPBO3RV"
      },
      "outputs": [],
      "source": [
        "output = {}\n",
        "\n",
        "for d in new_list:\n",
        "    d = json.loads(d)\n",
        "\n",
        "    for key, value in d.items():\n",
        "        output.setdefault(key, []).append(value)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mhXNmJAKO3RV"
      },
      "outputs": [],
      "source": [
        "output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0a3FIbq-O3RY",
        "outputId": "a7273913-b683-4e1d-f213-95bee7755034"
      },
      "outputs": [],
      "source": [
        "len(output['data'])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Here is the filter where you can see the values I would like to retrieve:\n",
        "\n",
        "output['data'][1]['search_by_raw_query']#['search_timeline']['timeline']['instructions']#[0][\"entries\"]#[0][\"content\"][\"itemContent\"]['tweet_results'][\"result\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6w5qowAcO3RZ"
      },
      "outputs": [],
      "source": [
        "# FINAL PARSE\n",
        "# This captures lots of duplicates, but then you can just drop them\n",
        "# You can see it hits some errors, but not many...\n",
        "# I printed the error lines so you can inspect them\n",
        "\n",
        "adv_entries = []\n",
        "\n",
        "counter = 1\n",
        "\n",
        "\n",
        "for entry in output[\"data\"]:\n",
        "  \n",
        "    ent = entry['search_by_raw_query']['search_timeline']['timeline']['instructions']\n",
        "\n",
        "    for e in ent:\n",
        "        try:\n",
        "            e1 = e[\"entries\"]\n",
        "\n",
        "            for e2 in e1:\n",
        "                adv_entries.append(e2[\"content\"][\"itemContent\"][\"tweet_results\"][\"result\"][\"legacy\"]) # usin legacy here helps narrow down the number of columns you have to deal with\n",
        "                print(\"good\")\n",
        "        except:\n",
        "\n",
        "            print(e2)\n",
        "            pass\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v8DuLVDYO3Rb",
        "outputId": "25d15407-fc49-4498-fb37-3807c090f953"
      },
      "outputs": [],
      "source": [
        "len(adv_entries)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Putting into DataFrame"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nwEOvGvhO3Rd"
      },
      "outputs": [],
      "source": [
        "df = pd.json_normalize(adv_entries)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZKnzh6WkO3Rd",
        "outputId": "4793e111-4a4d-43ce-c0bb-6004c738e449"
      },
      "outputs": [],
      "source": [
        "df.head(3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7x6mnk7jO3Rf",
        "outputId": "e632091d-a44a-42d6-aa12-61d3cef55fc1"
      },
      "outputs": [],
      "source": [
        "# We still have loads of columns!\n",
        "for col in df.columns:\n",
        "    print(col)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Checking for unique entries, to get rid of duplicates\n",
        "df.id_str.nunique()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Checking for unique entries, to get rid of duplicates\n",
        "df.full_text.nunique()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Drop duplicates based on which column you think most likely is unique, and still captures all your tweets\n",
        "# I choose id_str here\n",
        "\n",
        "df.drop_duplicates(subset=\"id_str\", inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Printing texts so I can check if the error texts we got above are in here\n",
        "\n",
        "for index, row in df.iterrows():\n",
        "    print(row.full_text)\n",
        "    print(\"---------------------------------------------------------\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# DROP EXTRA COLUMNS HERE! Be sure to keep anything you make want in the future!\n",
        "\n",
        "# With that many columns, dropping a column would take too long. You can instead create a new df of just columns you want\n",
        "# Tons of other ways to do this, but this is simpliest\n",
        "\n",
        "# df_new = df[[list of columns I want]]\n",
        "\n",
        "# Example\n",
        "# wanted_columns = ['A','D']\n",
        "# new_dataset = dataset[wanted_columns]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# If you are collecting daily, you would concat/merge/join your dataframes here."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Save"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yNi23SeNO3Rg"
      },
      "outputs": [],
      "source": [
        "PATH = \"../DATA/\"\n",
        "FILENAME = \"twitter_advanced.csv\" # may wish to use datetime to string and concat to make filename dynamic based on last scrape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oTMErGgwO3Rg"
      },
      "outputs": [],
      "source": [
        "df.to_csv(PATH + FILENAME, index=False)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "c_venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.6"
    },
    "orig_nbformat": 4
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
